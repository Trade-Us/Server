{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "- Activation Functions\n",
    "- Data PreProcessing\n",
    "- Weight Initialization\n",
    "- Batch Normalization\n",
    "- Babysitting the Learning Process\n",
    "- Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "1. Sigmoid $\\sigma(x) = \\cfrac{1}{1+e^{-x}}$\n",
    "2. tanh $tanh(x)$\n",
    "3. ReLU $max(0, x)$\n",
    "4. Leaky ReLU $max(0.1x, x)$\n",
    "5. ELU \n",
    "6. Maxout $max(w_1^Tx + b_1, w_2^Tx + b_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid\n",
    "\n",
    "sigmoid $\\sigma(x) = \\cfrac{1}{1+e^{-x}}$ Graph\n",
    "![graph_sigmoid](resource/grp_sigmoid.png)\n",
    "- Feature\n",
    "    - 숫자를 0~1 의 범위로 좁힌다.\n",
    "    - 극단적인 값을 가지는 neuron에 대하여 유의미한 해석이 가능하다. (이진분류)  \n",
    "\n",
    "\n",
    "- Drawback\n",
    "    - 극단적인 값을 가지는 neuron은 gradients를 저하시킨다.  \n",
    "    \n",
    "    극단적인 neuron값을 가질경우 위 그래프에서 알 수 있듯이 gradient가 0이 되기 때문에 Back Propagation을 적용할 수 없게 된다.  \n",
    "    \n",
    "    - Output 이 zero-centered 하지 않다.\n",
    "    \n",
    "    위 그래프와 같이 Output이 0이 중심이 아닌 것, 즉 zero mean data 형식을 가지지 않는 것을 말한다.  \n",
    "    이때, x 값이 항상 positive 한 경우 w(가중치)의 gradient 값이 모두 positive 이거나 negative 하게 된다는 문제가 발생한다. 이는 DF/Dw 는 x의 값이고 DL/Dw = DF/Dw x DL/DF으로 w의 gradient는 x의 값에 따라 좌우되기 때문이다.  \n",
    "    따라서, local gradient 에 의해 w의 gradient 는 정해지므로 update가 같은 방향으로 이루어지게 된다.[(+) -> (+) / (-) -> (-)] 이는 아래 그램과 같은 비효율적인 결과를 내게 되는 것이다. \n",
    "    ![zigzagpath](resource/zigzagpath.png)\n",
    "    \n",
    "    - exponential 계산이 비효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tanh\n",
    "\n",
    "tanh(x) graph\n",
    "![tanh](resource/grp_tanh.png)\n",
    "- Feature\n",
    "    - 숫자를 -1~1 의 범위로 좁힌다.\n",
    "    - zero centered 의 형식을 가진다!\n",
    "\n",
    "\n",
    "- DrawBack\n",
    "    - 극단적인 값을 가지는 neuron은 gradients를 소멸시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "**Want unit gaussian(정규분포) activations? Make them so**\n",
    "\n",
    "Apply this:  \n",
    "$$\\hat{x}^{(k)} = \\cfrac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$$\n",
    "\n",
    "- Feature\n",
    "    - Weight Initialize 대신에 데이터를 정규분포화 시키므로써 결과가 더 잘 나오도록 함 (every layer 마다 시행)\n",
    "    - 매번의 Layer마다 발생하는 Bad Scaling을 이를 통해 해결할 수 있다.\n",
    "    - FC(Fully Connected) Or Convolutional layer 다음에 정규분포화 하는데, Convolutional layer의 경우 각 Activaion map 마다 적용되는 것이다.\n",
    "    \n",
    "  \n",
    "- How? N 개의 Examples / D 개의 차원 수 \n",
    "    1. 각 차원마다 평균과 분산을 경험적으로 계산한다.\n",
    "    2. 위 식을 통해 Normalize 한다.\n",
    "    \n",
    "    \n",
    "*tanh에 반드시 적용해야 할 필요가 있나?*\n",
    "\n",
    "Yes.. Normalization을 통해 Data를 Squash or Shift 할 수 있다! 즉, tanh는 적절한 x 구간에서 다양한 output을 내는데, 정규분포화를 통해서 해당 영역 안으로 데이터들을 모아주면 더 좋은 결과를 내게 된다. \n",
    "\n",
    "\n",
    "정규분포 그래프\n",
    "![NormalDistribution](resource/NormalDistribution.png)\n",
    "\n",
    "$$y^{(k)} = \\gamma^{(k)}\\hat{x} + \\beta^{(k)}$$\n",
    "\n",
    "**Note! The Network can learn:**  \n",
    "$\\gamma^{(k)} = \\sqrt{Var[x^{(k)}]}$  (Squash)  \n",
    "$\\beta^{(k)} = E[x^{(k)}]$  (Shift)\n",
    "\n",
    "\n",
    "Summary\n",
    "- Improves gradient flow through the network\n",
    "- Allows higher learning rates\n",
    "- Reduces the strong dependence on initialization\n",
    "- Acts as a form of regularization\n",
    "- in a funny way, and slightly reduces the need for dropout, maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babysitting the Learning Process\n",
    "\n",
    "**Steps:**  \n",
    "1. Preprocess the data\n",
    "\n",
    "\n",
    "2. Choose the architecture\n",
    "\n",
    "\n",
    "3. Double check that the loss is reasonable\n",
    "\n",
    "\n",
    "4. Try to train\n",
    "\n",
    "*point*  \n",
    "loss 를 살피면서 learning rate를 올리거나 내려준다.(BabySitting)  \n",
    "LR 이 너무 높으면, loss exploding  \n",
    "LR 이 너무 낮으면, loss not going down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "**Cross-validation Startegy**  \n",
    "\n",
    "coarse(Rough!) 대략적인 param의 범위 유추 => 검증셋을 통한 정제화(fine) param 찾아냄  \n",
    "\n",
    "**First Stage:** a Few epochs! params가 어떤지에 대한 대략적인 아이디어를 확인한다.  \n",
    "**Second Stage:** 긴시간을 통해 연구를 정제화 한다. \n",
    "\n",
    "정형화된 Grid Search 보다 Random Search 가 더 좋은 value 를 얻을 확률이 높다!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
