{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "- Activation Functions\n",
    "- Data PreProcessing\n",
    "- Weight Initialization\n",
    "- Batch Normalization\n",
    "- Babysitting the Learning Process\n",
    "- Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "1. Sigmoid $\\sigma(x) = \\cfrac{1}{1+e^{-x}}$\n",
    "2. tanh $tanh(x)$\n",
    "3. ReLU $max(0, x)$\n",
    "4. Leaky ReLU $max(0.1x, x)$\n",
    "5. ELU \n",
    "6. Maxout $max(w_1^Tx + b_1, w_2^Tx + b_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid\n",
    "\n",
    "sigmoid $\\sigma(x) = \\cfrac{1}{1+e^{-x}}$ Graph\n",
    "![graph_sigmoid](resource/lec6/grp_sigmoid.PNG)\n",
    "- Feature\n",
    "    - 숫자를 0~1 의 범위로 좁힌다.\n",
    "    - 극단적인 값을 가지는 neuron에 대하여 유의미한 해석이 가능하다. (이진분류)  \n",
    "\n",
    "\n",
    "- Drawback\n",
    "    - 극단적인 값을 가지는 neuron은 gradients를 저하시킨다.  \n",
    "    \n",
    "    극단적인 neuron값을 가질경우 위 그래프에서 알 수 있듯이 gradient가 0이 되기 때문에 Back Propagation을 적용할 수 없게 된다.  \n",
    "    \n",
    "    - Output 이 zero-centered 하지 않다.\n",
    "    \n",
    "    위 그래프와 같이 Output이 0이 중심이 아닌 것, 즉 zero mean data 형식을 가지지 않는 것을 말한다.  \n",
    "    이때, x 값이 항상 positive 한 경우 w(가중치)의 gradient 값이 모두 positive 이거나 negative 하게 된다는 문제가 발생한다. 이는 DF/Dw 는 x의 값이고 DL/Dw = DF/Dw x DL/DF으로 w의 gradient는 x의 값에 따라 좌우되기 때문이다.  \n",
    "    따라서, local gradient 에 의해 w의 gradient 는 정해지므로 update가 같은 방향으로 이루어지게 된다.[(+) -> (+) / (-) -> (-)] 이는 아래 그램과 같은 비효율적인 결과를 내게 되는 것이다. \n",
    "    ![zigzagpath](resource/lec6/zigzagpath.PNG)\n",
    "    \n",
    "    - exponential 계산이 비효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tanh\n",
    "\n",
    "tanh(x) graph\n",
    "![tanh](resource/lec6/grp_tanh.PNG)\n",
    "- Feature\n",
    "    - 숫자를 -1~1 의 범위로 좁힌다.\n",
    "    - zero centered 의 형식을 가진다!\n",
    "\n",
    "\n",
    "- DrawBack\n",
    "    - 극단적인 값을 가지는 neuron은 gradients를 소멸시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ReLU\n",
    "\n",
    "graph of ReLU\n",
    "![grpRelu](resource/lec6/grp_relu.png)\n",
    "\n",
    "가장 대중적으로 많이 사용하는 activation function이다. ReLU는 0이하인 값들은 전부 0으로 그 이상인 값들은 그 값 그대로 출력해주는 함수다. F(x) = max(0,x)의 식을 가지며 매우 단순하지만 잘 동작하는 activation function이다.  \n",
    "\n",
    "하지만 문제점이 존재한다.  \n",
    "\n",
    "0이하의 값들은 모두 버리게 되어 dead ReLU에 빠지는 문제와 zero-centered가 되어 있지 않다는 문제가 있다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Leaky ReLU\n",
    "\n",
    "graph of Leaky ReLU\n",
    "![grpLeakyrelu](resource/lec6/grp_leakyrelu.png)\n",
    "\n",
    "ReLU의 문제를 보완하고자 나온 activation function 이다.  \n",
    "\n",
    "0이하의 값을 0.01x값을 줘서 작은 값이라도 출력될 수 있게 한다. 이와 비슷한 PReLU라는 함수도 존재한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ELU\n",
    "\n",
    "graph of ELU\n",
    "![grp_elu](resource/lec6/grp_elu.png)\n",
    "\n",
    "ReLU의 모든 장점을 가지고 있고 zero mean과 가까운 결과가 나오게 된다. 하지만 exp계산을 해야하는 것 가장 큰 장점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Maxout \"Neuron\"\n",
    "\n",
    "Max()값을 통해 2개의 파라미터를 비교하여 더 좋은 것을 선택하는 것   \n",
    "연산량이 2배가 더 늘어나기 때문에 잘 사용하지 않는다.  \n",
    "\n",
    "**일반적으로 딥러닝에서는 ReLU와 Leaky ReLU를 많이 사용한다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before we start training with our input data, we have to preprocess the data for better learning.  \n",
    "\n",
    "We do the preprocessing in the training phase and also in the test phase. We apply this exact same mean we determined in the training phase to the test data  \n",
    "\n",
    "For images, we just do the zero-centered pre-processing\n",
    "\n",
    "1. zero-centering \n",
    "\n",
    "\n",
    "2. normalization\n",
    "\n",
    "\n",
    "3. PCA, Whitening: more complicated pre-processing in machine learning (we don’t need to do with images\n",
    "\n",
    "Data Preprocessing\n",
    "![datapreprocess](resource/lec6/datapreprocess.png)\n",
    "\n",
    "1. zero-centering : 각각에 대해 평균값을 빼줌\n",
    "\n",
    "zero-mean the data and subtract each data  \n",
    "\n",
    "![zigzagpath](resource/lec6/zigzagpath.png)\n",
    "\n",
    "\n",
    "$$f(\\sum_{i}{w_ix_i + b})$$\n",
    "\n",
    "if our inputs are not zero-centered, we get all of our gradients on the weights to be positive or negative and we get this basically suboptimal optimization   \n",
    "\n",
    "2. Normalization (정규화): 표준편차를 나눠 줌\n",
    "\n",
    "so that all features are in the same range, and so that they contribute equally.  \n",
    "\n",
    "Usually used in machine learning problems, where you might have different features that are very different and of very different scales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "**Want unit gaussian(정규분포) activations? Make them so**\n",
    "\n",
    "Apply this:  \n",
    "$$\\hat{x}^{(k)} = \\cfrac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$$\n",
    "\n",
    "- Feature\n",
    "    - Weight Initialize 대신에 데이터를 정규분포화 시키므로써 결과가 더 잘 나오도록 함 (every layer 마다 시행)\n",
    "    - 매번의 Layer마다 발생하는 Bad Scaling을 이를 통해 해결할 수 있다.\n",
    "    - FC(Fully Connected) Or Convolutional layer 다음에 정규분포화 하는데, Convolutional layer의 경우 각 Activaion map 마다 적용되는 것이다.\n",
    "    \n",
    "  \n",
    "- How? N 개의 Examples / D 개의 차원 수 \n",
    "    1. 각 차원마다 평균과 분산을 경험적으로 계산한다.\n",
    "    2. 위 식을 통해 Normalize 한다.\n",
    "    \n",
    "    \n",
    "*tanh에 반드시 적용해야 할 필요가 있나?*\n",
    "\n",
    "Yes.. Normalization을 통해 Data를 Squash or Shift 할 수 있다! 즉, tanh는 적절한 x 구간에서 다양한 output을 내는데, 정규분포화를 통해서 해당 영역 안으로 데이터들을 모아주면 더 좋은 결과를 내게 된다. \n",
    "\n",
    "\n",
    "정규분포 그래프\n",
    "![NormalDistribution](resource/lec6/NormalDistribution.png)\n",
    "\n",
    "$$y^{(k)} = \\gamma^{(k)}\\hat{x} + \\beta^{(k)}$$\n",
    "\n",
    "**Note! The Network can learn:**  \n",
    "$\\gamma^{(k)} = \\sqrt{Var[x^{(k)}]}$  (Squash)  \n",
    "$\\beta^{(k)} = E[x^{(k)}]$  (Shift)\n",
    "\n",
    "\n",
    "Summary\n",
    "- Improves gradient flow through the network\n",
    "- Allows higher learning rates\n",
    "- Reduces the strong dependence on initialization\n",
    "- Acts as a form of regularization\n",
    "- in a funny way, and slightly reduces the need for dropout, maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babysitting the Learning Process\n",
    "\n",
    "**Steps:**  \n",
    "1. Preprocess the data\n",
    "\n",
    "\n",
    "2. Choose the architecture\n",
    "\n",
    "\n",
    "3. Double check that the loss is reasonable\n",
    "\n",
    "\n",
    "4. Try to train\n",
    "\n",
    "*point*  \n",
    "loss 를 살피면서 learning rate를 올리거나 내려준다.(BabySitting)  \n",
    "LR 이 너무 높으면, loss exploding  \n",
    "LR 이 너무 낮으면, loss not going down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "**Cross-validation Startegy**  \n",
    "\n",
    "coarse(Rough!) 대략적인 param의 범위 유추 => 검증셋을 통한 정제화(fine) param 찾아냄  \n",
    "\n",
    "**First Stage:** a Few epochs! params가 어떤지에 대한 대략적인 아이디어를 확인한다.  \n",
    "**Second Stage:** 긴시간을 통해 연구를 정제화 한다. \n",
    "\n",
    "정형화된 Grid Search 보다 Random Search 가 더 좋은 value 를 얻을 확률이 높다!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
